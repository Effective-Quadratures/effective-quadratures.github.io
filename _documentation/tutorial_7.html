
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Data-driven dimension reduction in turbomachinery &#8212; equadratures</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/eq-logo-favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Adjoint/ gradient-enhanced surrogate modelling" href="tutorial_8.html" />
    <link rel="prev" title="Sensitivity analysis for a piston model" href="tutorial_6.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../index.html">
    
      <img src="../_static/logo_new.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item active">
            <a class="nav-link" href="tutorials.html">Tutorials</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="modules.html">Modules</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="references.html">Research</a>
        </li>
        
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discourse.equadratures.org/">Discourse<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>


      <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search equadratures..." aria-label="Search equadratures..." autocomplete="off" >
</form>
      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/Effective-Quadratures/equadratures" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
          <li class="nav-item">
            <a class="nav-link" href="https://twitter.com/equadratures" target="_blank" rel="noopener">
              <span><i class="fab fa-twitter-square"></i></span>
            </a>
          </li>
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
          
            
                <li class="">
                    <a href="tutorial_1.html">Foundations I: A parameter</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_2.html">Foundations II: Orthogonal polynomials</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_3.html">Foundations III: Solving linear systems for polynomials</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_4.html">Uncertainty quantification in computational fluid dynamics</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_5.html">Uncertainty quantification with correlations</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_6.html">Sensitivity analysis for a piston model</a>
                </li>
            
          
            
                <li class="active">
                    <a href="">Data-driven dimension reduction in turbomachinery</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_8.html">Adjoint/ gradient-enhanced surrogate modelling</a>
                </li>
            
          
            
                <li class="">
                    <a href="tutorial_9.html">Surrogate-based design optimisation on Rosenbrock’s function</a>
                </li>
            
          
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#random-projections" class="nav-link">Random projections</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#active-subspaces" class="nav-link">Active subspaces</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#polynomial-variable-projection" class="nav-link">Polynomial variable projection</a>
        </li>
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="data-driven-dimension-reduction-in-turbomachinery">
<h1>Data-driven dimension reduction in turbomachinery<a class="headerlink" href="#data-driven-dimension-reduction-in-turbomachinery" title="Permalink to this headline">¶</a></h1>
<div class="section" id="random-projections">
<h2>Random projections<a class="headerlink" href="#random-projections" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we present a Bayesian analogue to polynomial regression; the material presented here is heavily derived  from Chapter 3.8 in Rogers and Girolami. We will aim to fit a Bayesian polynomial regression model to the data we considered in the prior tutorial. Bayesian approaches are characterized by four key elements: the model, the prior, the likelihood and the posterior.</p>
<p><strong>The model</strong></p>
<p>We define our model as</p>
<div class="math notranslate nohighlight">
\[\mathbf{f} = \mathbf{Ax} + \boldsymbol{\epsilon},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{N \times M}\)</span> is our <em>Vandermonde</em> type matrix, comprising of <span class="math notranslate nohighlight">\(M\)</span> orthogonal polynomials evaluated at the <span class="math notranslate nohighlight">\(N\)</span> <em>sample inputs</em>. In other words each entry of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> can be given by</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}_{ij} = \psi_i \left(  \lambda_j \right).\]</div>
<p>Returning back to the first equation, we have <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} \in \mathbb{R}^{N}\)</span> which is the noise associated with each <em>sample output</em>, i.e., each element of the vector. To simplify matters, we introduce the standard Gaussian assumption for the noise</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\epsilon} = \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span> indicates the zero vector and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> denotes the covariance matrix associated with the noisy measurements. We assume that these are known. Infact we will assume that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the identity matrix; we set <span class="math notranslate nohighlight">\(\sigma=1.5\)</span>.</p>
<p><strong>The likelihood</strong></p>
<p>In a Bayesian context, the likelihood can be interpreted as a model of some parameters given data. It is formed from the joint probability of both the same data and its associated model parameters. Our likelihood function here is given by</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) = \mathcal{N} \left( \mathbf{Ax}, \sigma^2 \mathbf{I}     \right).\]</div>
<p><strong>The prior</strong></p>
<p>One of the goals of polynomial regression is to ascertain the value of our coefficients <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As we would like an exact expression for the posterior, as Rogers and Girolami state, we require a prior that is <strong>conjugate</strong> to the Gaussian likelihood above. The simplest choice for facilitating this conjugacy is to ensure that our prior too is Gaussian. We therefore write our prior as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right) = \mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right),\]</div>
<p>where we set <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \boldsymbol{0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{0} = 10 \mathbf{I}\)</span>. The high-level idea is that we generate random coefficients—sampling from <span class="math notranslate nohighlight">\(\mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right)\)</span>—for the first six Legendre polynomials and plot them!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">myBasis</span> <span class="o">=</span> <span class="n">Basis</span><span class="p">(</span><span class="s1">&#39;Univariate&#39;</span><span class="p">)</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">myParameters</span><span class="p">,</span> <span class="n">myBasis</span><span class="p">)</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># Extracting the Vandermonde-type matrix!</span>

<span class="c1"># Define the prior!</span>
<span class="n">mu_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Sigma_0</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">Sigma_0</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_a.png"><img alt="../_images/tutorial_7_fig_a.png" src="../_images/tutorial_7_fig_a.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples from the prior distribution (grey lines) assigned to the polynomial coefficients.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p><strong>The posterior</strong></p>
<p>But clearly this isn’t too informative. What we want is to posterior! The posterior can be expressed as a product of the likelihood and the prior. In this tutorial, we know our posterior will also be a Gaussian distribution. What is not known is its precise mean and covariance. Using Bayes’ rule, one can express the posterior distribution as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \mathbf{f}, \mathbf{A}, \sigma^2  \right) \propto  p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right),\]</div>
<p>which upon simplification yields</p>
<div class="math notranslate nohighlight">
\[= \text{exp} \left\{  -\frac{1}{2} \left( \frac{1}{\sigma^2} \left( \mathbf{f} - \mathbf{Ax}  \right)^{T}  \left( \mathbf{f} - \mathbf{Ax}  \right)  +   \left( \mathbf{x} - \boldsymbol{\mu} \right)^{T}  \boldsymbol{\Sigma}_{0}^{-1}  \left( \mathbf{x} - \boldsymbol{\mu} \right)     \right)     \right\}.\]</div>
<p>The covariance and the mean can then be given as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_{\mathbf{x}} = \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{A}  + \boldsymbol{\Sigma}^{-1}_{0}    \right)^{-1}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_{\mathbf{x}} = \boldsymbol{\Sigma}_{\mathbf{x}} \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{f} + \boldsymbol{\Sigma}_{0}^{-1} \boldsymbol{\mu}_{0}   \right)\]</div>
<p>respectively.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span> <span class="c1"># only selecting 4 sample points!</span>
<span class="n">x_use</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">y_use</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">Sigma_measurements</span> <span class="o">=</span> <span class="n">noise</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>
<span class="n">P_data</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_use</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># and now computing the posterior covariance and mean...</span>
<span class="n">Sigma_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span> <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">P_data</span>  <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_0</span><span class="p">)</span> <span class="p">)</span>
<span class="n">mu_x</span> <span class="o">=</span> <span class="n">Sigma_x</span> <span class="o">@</span>   <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">y_use</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Sigma_x</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_b.png"><img alt="../_images/tutorial_7_fig_b.png" src="../_images/tutorial_7_fig_b.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples created from the coefficients drawn from the posterior after observing four data points. The mean is shown in blue.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>The full source code for this tutorial can be found <a class="reference external" href="https://github.com/Effective-Quadratures/Effective-Quadratures/blob/master/source/_documentation/codes/tutorial_7.py">here.</a></p>
</div>
<div class="section" id="active-subspaces">
<h2>Active subspaces<a class="headerlink" href="#active-subspaces" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="polynomial-variable-projection">
<h2>Polynomial variable projection<a class="headerlink" href="#polynomial-variable-projection" title="Permalink to this headline">¶</a></h2>
<p><strong>References</strong></p>
<ul class="simple">
<li><p>Rogers, S., Girolami, M. (2016). A First Course in Machine Learning. Chapman and Hall/CRC, Boca Raton, Florida, U.S.A.</p></li>
</ul>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2016-2021 by Effective Quadratures.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.4.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>